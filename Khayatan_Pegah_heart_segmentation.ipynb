{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE3D_oi_-ECZ",
        "outputId": "e5d8d858-daa9-4a1f-b336-b963711256ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchio\n",
            "  Downloading torchio-0.18.91-py2.py3-none-any.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.8/172.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from torchio) (4.6.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.10/dist-packages (from torchio) (2.0.0+cu118)\n",
            "Collecting SimpleITK!=2.0.*,!=2.1.1.1\n",
            "  Downloading SimpleITK-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchio) (4.65.0)\n",
            "Collecting Deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from torchio) (3.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torchio) (1.10.1)\n",
            "Requirement already satisfied: typer[all] in /usr/local/lib/python3.10/dist-packages (from torchio) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from torchio) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1->torchio) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1->torchio) (3.25.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->torchio) (1.14.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]->torchio) (8.1.3)\n",
            "Collecting rich<13.0.0,>=10.11.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama<0.5.0,>=0.4.3\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0\n",
            "  Downloading shellingham-1.5.0.post1-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0.0,>=10.11.0->typer[all]->torchio) (2.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1->torchio) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1->torchio) (1.3.0)\n",
            "Installing collected packages: SimpleITK, commonmark, shellingham, rich, Deprecated, colorama, torchio\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.3.4\n",
            "    Uninstalling rich-13.3.4:\n",
            "      Successfully uninstalled rich-13.3.4\n",
            "Successfully installed Deprecated-1.2.13 SimpleITK-2.2.1 colorama-0.4.6 commonmark-0.9.1 rich-12.6.0 shellingham-1.5.0.post1 torchio-0.18.91\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 43.0 kB of archives.\n",
            "After this operation, 115 kB of additional disk space will be used.\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
            "Unpacking tree (1.8.0-1) ...\n",
            "Setting up tree (1.8.0-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install torchio\n",
        "%pip install --quiet matplotlib\n",
        "%pip install --quiet torchvision\n",
        "!apt -qq install tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4HyLJI0-RYG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import enum\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchio as tio\n",
        "from torch.utils.data import DataLoader\n",
        "import multiprocessing\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import ConcatDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "\n",
        "\n",
        "num_workers = multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpDyhvwo-MgN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4de6e0-f061-4bbd-b313-e249e106fe8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  7 21:00:17 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jC3q7y4-SDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c38391-70b9-4b02-907f-5988ef3d622e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Errno 2] No such file or directory: '/gdrive'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjyMgSzO-UWR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/MyDrive/IMA205/ima205-challenge-2023\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L5bAf5w-WrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84e0082-d003-4901-c148-4be2a425a475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".DS_Store\n",
            "Classif_model.pt\n",
            "SampleSubmission.csv\n",
            "Segmentation_model_.pt\n",
            "Test\n",
            "Train\n",
            "best_models\n",
            "metaDataTest.csv\n",
            "metaDataTrain.csv\n",
            "new_segmentation_models\n",
            "results\n",
            "segmentation_model_ED.pt\n",
            "segmentation_model_ES.pt\n"
          ]
        }
      ],
      "source": [
        "data_path = \"/content/drive/MyDrive/IMA205/ima205-challenge-2023\"\n",
        "\n",
        "\n",
        "for file in sorted(os.listdir(data_path)):\n",
        "  print(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_meta_test = pd.read_csv(os.path.join(data_path, 'metaDataTest.csv'))\n",
        "df_meta_train = pd.read_csv(os.path.join(data_path, 'metaDataTrain.csv'))\n"
      ],
      "metadata": {
        "id": "oNYZgzYQfXm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc5F4pWq-YkJ"
      },
      "outputs": [],
      "source": [
        "training_transform = tio.Compose([\n",
        "    #tio.ToCanonical(),\n",
        "    tio.Resample((1.25, 1.25, 10)),\n",
        "    #tio.RandomMotion(p=0.2),\n",
        "    tio.CropOrPad((200, 200, 10)),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "    #tio.OneHot(),\n",
        "])\n",
        "\n",
        "validation_transform = tio.Compose([\n",
        "    #tio.ToCanonical(),\n",
        "    tio.Resample((1.25, 1.25, 10)),\n",
        "    tio.CropOrPad((200, 200, 10)),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "    #tio.OneHot(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc3nGm5J-cSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4e1573-02a4-4e88-a480-53235103a725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 200 subjects\n"
          ]
        }
      ],
      "source": [
        "# create a dataset of first type of mri and the second one\n",
        "from random import shuffle\n",
        "\n",
        "def counter(a):\n",
        "  if a >= 100:\n",
        "    return str(a)\n",
        "  elif a < 10:\n",
        "    return '00'+str(a)\n",
        "  else:\n",
        "    return '0'+str(a)\n",
        "\n",
        "\n",
        "#subjects_ED = []\n",
        "#subjects_ES = []\n",
        "\n",
        "subjects_mixed = []\n",
        "\n",
        "\n",
        "permute = np.random.permutation(100)+1\n",
        "\n",
        "\n",
        "for i in permute:\n",
        "  current_dir = os.path.join(data_path, 'Train', counter(i))\n",
        "  subject_ED = tio.Subject(\n",
        "        mri=tio.ScalarImage(os.path.join(current_dir, counter(i)+'_ED.nii')),\n",
        "        segment=tio.LabelMap(os.path.join(current_dir, counter(i)+'_ED_seg.nii')),\n",
        "        height=df_meta_train.iloc[i-1]['Height'],\n",
        "        weight=df_meta_train.iloc[i-1]['Weight'],\n",
        "        category=df_meta_train.iloc[i-1]['Category']\n",
        "\n",
        "    )\n",
        "\n",
        "  subject_ES = tio.Subject(\n",
        "        mri=tio.ScalarImage(os.path.join(current_dir, counter(i)+'_ES.nii')),\n",
        "        segment=tio.LabelMap(os.path.join(current_dir, counter(i)+'_ES_seg.nii')),\n",
        "        height=df_meta_train.iloc[i-1]['Height'],\n",
        "        weight=df_meta_train.iloc[i-1]['Weight'],\n",
        "        category=df_meta_train.iloc[i-1]['Category']\n",
        "    )\n",
        "\n",
        "\n",
        "  subjects_mixed.append(subject_ED)\n",
        "  subjects_mixed.append(subject_ES)\n",
        "\n",
        "\n",
        "shuffle(subjects_mixed)\n",
        "\n",
        "\n",
        "\n",
        "dataset_train = tio.SubjectsDataset(subjects_mixed, transform=training_transform)\n",
        "\n",
        "'''\n",
        "lengths = [int(len(dataset_ED_train)*0.8), int(len(dataset_ED_train)*0.2)]\n",
        "train_ED, valid_ED = torch.utils.data.random_split(dataset_ED_train, lengths)\n",
        "lengths = [int(len(dataset_ES_train)*0.8), int(len(dataset_ES_train)*0.2)]\n",
        "train_ES, valid_ES = torch.utils.data.random_split(dataset_ES_train, lengths)\n",
        "'''\n",
        "print('Dataset size:', len(dataset_train), 'subjects')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxsiNJmkasJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3067c21f-4d1f-4a02-b08a-fabe5d05f210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 50 subjects\n"
          ]
        }
      ],
      "source": [
        "# create a dataset of first type of mri and the second one\n",
        "\n",
        "subjects_ED = []\n",
        "subjects_ES = []\n",
        "for i in range(101,151):\n",
        "  current_dir = os.path.join(data_path, 'Test', counter(i))\n",
        "  subject_ED = tio.Subject(\n",
        "        mri=tio.ScalarImage(os.path.join(current_dir, counter(i)+'_ED.nii')),\n",
        "        segment=tio.LabelMap(os.path.join(current_dir, counter(i)+'_ED_seg.nii')),\n",
        "        height=df_meta_test.iloc[i-100-1]['Height'],\n",
        "        weight=df_meta_test.iloc[i-100-1]['Weight'],\n",
        "    )\n",
        "  subject_ES = tio.Subject(\n",
        "        mri=tio.ScalarImage(os.path.join(current_dir, counter(i)+'_ES.nii')),\n",
        "        segment=tio.LabelMap(os.path.join(current_dir, counter(i)+'_ES_seg.nii')),\n",
        "        height=df_meta_test.iloc[i-100-1]['Height'],\n",
        "        weight=df_meta_test.iloc[i-100-1]['Weight'],\n",
        "    )\n",
        "\n",
        "  subjects_ED.append(subject_ED)\n",
        "  subjects_ES.append(subject_ES)\n",
        "\n",
        "\n",
        "\n",
        "dataset_ED_test = tio.SubjectsDataset(subjects_ED, transform=validation_transform)\n",
        "dataset_ES_test = tio.SubjectsDataset(subjects_ES, transform=validation_transform)\n",
        "\n",
        "\n",
        "print('Dataset size:', len(dataset_ED_test), 'subjects')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MskjbA8ap8X"
      },
      "outputs": [],
      "source": [
        "training_batch_size = 4\n",
        "validation_batch_size = 4\n",
        "\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=training_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        ")\n",
        "\n",
        "\n",
        "test_ED_loader = torch.utils.data.DataLoader(\n",
        "    dataset_ED_test,\n",
        "    batch_size=validation_batch_size,\n",
        "    num_workers=num_workers)\n",
        "\n",
        "test_ES_loader = torch.utils.data.DataLoader(\n",
        "    dataset_ES_test,\n",
        "    batch_size=validation_batch_size,\n",
        "    num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p9AUsvHbQ21"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the network architecture\n",
        "class DilatedConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DilatedConvNet, self).__init__()\n",
        "\n",
        "\n",
        "        self.encoder_1 = nn.Sequential(nn.Conv3d(1, 4, kernel_size=(5,5,1), padding=(2,2,0), dilation=(1,1,1)),\n",
        "                                     nn.BatchNorm3d(4),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.MaxPool3d((2, 2, 1), stride=(2, 2, 1))) # 100\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(nn.Conv3d(4, 8, kernel_size=(5,5,1), padding=(2,2,0), dilation=(1,1,1)),\n",
        "                                     nn.BatchNorm3d(8),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.MaxPool3d((2, 2, 1), stride=(2, 2, 1))) # 50\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(nn.Conv3d(8, 16, kernel_size=(5,5,1), padding=(2,2,0), dilation=(1,1,1)),\n",
        "                                     nn.BatchNorm3d(16),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.MaxPool3d((2, 2, 1), stride=(2, 2, 1))) # 25\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(nn.Conv3d(16, 32, kernel_size=(5,5,1), padding=(2,2,0), dilation=(1,1,1)),\n",
        "                                     nn.BatchNorm3d(32),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.MaxPool3d((2, 2, 1), stride=(2, 2, 1))) # 12\n",
        "\n",
        "        self.encoder_5 = nn.Sequential(nn.Conv3d(32, 64, kernel_size=(5,5,1), padding=(2,2,0), dilation=(1,1,1)),\n",
        "                                     nn.BatchNorm3d(64),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.MaxPool3d((2, 2, 1), stride=(2, 2, 1))) # (batch_size, 64, 6, 6)\n",
        "\n",
        "\n",
        "        # should use ConvTranspose3d to inverse the maxpooling\n",
        "        # (D_in−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+1\n",
        "\n",
        "        # 6 to 12 => 12 = 5*2 - 2*1 + 1*3 + 1\n",
        "        self.decoder_5 = nn.Sequential(nn.ConvTranspose3d(64, 32, (4, 4, 1), stride=(2, 2, 1), padding=(1, 1, 0)), # (batch_size,8,50,50,5)\n",
        "                                     nn.BatchNorm3d(32),\n",
        "                                     nn.ReLU(inplace=True)) # (batch_size, 32, 12, 12)\n",
        "        # we should combine the output of decoder_5 with the output of encoder_4 and pass it to\n",
        "        # decoder_4\n",
        "\n",
        "        # 12 to 25 => 25 = 11*2 - 2*1 + 1*4 + 1\n",
        "        self.decoder_4 = nn.Sequential(nn.ConvTranspose3d(64, 32, (5, 5, 1), stride=(2, 2, 1), padding=(1, 1, 0)), # (batch_size,8,50,50,5)\n",
        "                                     nn.BatchNorm3d(32),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.Conv3d(32, 16, kernel_size=(1,1,1)),\n",
        "                                     nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "        # 25 to 50 => 50 = 24*2 - 2*1 + 1*3 + 1\n",
        "        self.decoder_3 = nn.Sequential(nn.ConvTranspose3d(32, 16, (4, 4, 1), stride=(2, 2, 1), padding=(1, 1, 0)), # (batch_size,8,50,50,5)\n",
        "                                     nn.BatchNorm3d(16),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.Conv3d(16, 8, kernel_size=(1,1,1)),\n",
        "                                     nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "        # 50 to 100 => 50 = 24*2 - 2*1 + 1*3 + 1\n",
        "        self.decoder_2 = nn.Sequential(nn.ConvTranspose3d(16, 8, (4, 4, 1), stride=(2, 2, 1), padding=(1, 1, 0)), # (batch_size,8,50,50,5)\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.Conv3d(8, 4, kernel_size=(1,1,1)),\n",
        "                                     nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "        # 100 to 200 => 200 = 99*2 - 2*1 + 1*3 + 1\n",
        "        self.decoder_1 = nn.Sequential(nn.ConvTranspose3d(8, 4, (4, 4, 1), stride=(2, 2, 1), padding=(1, 1, 0)), # (batch_size,8,50,50,5)\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.Conv3d(4, 1, kernel_size=(1,1,1)),\n",
        "                                     #nn.ReLU(inplace=True)\n",
        "                                     )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input, mode='train'):\n",
        "        # Input shape: (batch_size, 281, 281, 10)\n",
        "        # we convert it to (batch_size, 3, 281, 281, 10)\n",
        "\n",
        "        seg_myo = (input['segment'][tio.DATA][:,0,:]==2).int().unsqueeze(1).to(device).float()\n",
        "\n",
        "\n",
        "        features = []\n",
        "        x = seg_myo\n",
        "        features.append(x)\n",
        "        x = self.encoder_1(x)\n",
        "        features.append(x)\n",
        "        x = self.encoder_2(x)\n",
        "        features.append(x)\n",
        "        x = self.encoder_3(x)\n",
        "        features.append(x)\n",
        "        x = self.encoder_4(x)\n",
        "        features.append(x)\n",
        "        x = self.encoder_5(x)\n",
        "\n",
        "\n",
        "        x = self.decoder_5(x)\n",
        "        #print(x.size())\n",
        "        x = torch.cat((x,features[-1]), dim=1)\n",
        "        #print(x.size())\n",
        "        x = self.decoder_4(x)\n",
        "        x = torch.cat((x,features[-2]), dim=1)\n",
        "        x = self.decoder_3(x)\n",
        "        x = torch.cat((x,features[-3]), dim=1)\n",
        "        x = self.decoder_2(x)\n",
        "        x = torch.cat((x,features[-4]), dim=1)\n",
        "        x = self.decoder_1(x)\n",
        "\n",
        "        output = x\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "# train a model for 300 epochs, where each epoch is 100 batches, each batch contains 4 samples\n",
        "# 300*100 = 30000\n",
        "# so, in each epoch, we go through the whole dataset (in average, but we impose this average in the code)\n",
        "# 4 times => n_steps_in_epoch\n",
        "def train(model, data_loader, epochs=300, n_sets_in_epoch=4, lr=0.005):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    schedualr = optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.98)\n",
        "    # scheduler = CyclicLR(optimizer, base_lr=0.2)\n",
        "\n",
        "    # Define the loss function and the device\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the model and the loss function to the device\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "    model.eval() if optimizer is None else model.train()\n",
        "    train_loss_list = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Train the model\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for n_set in range(n_sets_in_epoch):\n",
        "\n",
        "            for i, input in enumerate(data_loader):\n",
        "\n",
        "              target = (input['segment'][tio.DATA]==3).int().to(device).float()\n",
        "              optimizer.zero_grad()\n",
        "              output = model(input)\n",
        "              output_ = output.cpu()\n",
        "              output_ = output_[0,0,:,:,4]\n",
        "              print(output_.size())\n",
        "\n",
        "              plt.imshow(output_.detach().numpy())\n",
        "              plt.show()\n",
        "\n",
        "              loss = criterion(output, target)\n",
        "              print('loss in current batch ', loss.item())\n",
        "\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              schedualr.step()\n",
        "\n",
        "              train_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate the epoch loss\n",
        "        train_loss /= len(data_loader)\n",
        "        print('loss in the epoch ', train_loss)\n",
        "        train_loss_list.append(train_loss)\n",
        "\n",
        "\n",
        "    # Return the snapshot ensemble lists, the best model and its corresponding validation loss\n",
        "    return train_loss_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DilatedConvNet()\n",
        "train(model, training_loader)\n",
        "torch.save(model.state_dict(), os.path.join(data_path, 'new_segmentation_models', 'Segmentation_model_.pt'))"
      ],
      "metadata": {
        "id": "feaBxcyR8mGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}